{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Sergey Li-Zan-Men MADE\n\nПробовал ef2-7 и vit_b_32. Пробовал менять головы, но качество лучше чем 0.89797 на паблике не получил. Ресурсы ограниченны 30 часов в неделю на гпу кегла, поэтому очень много сабмитов (сохранял с разных эпох). Обидно не перейти за 0.9 на паблике (на привате перешел) и честно не понимаю, почему vit плохо сходился (лосс на трейне не падал ниже 3). Весь код тот же меняется только модель в основе","metadata":{}},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom sklearn.metrics import f1_score\n\nfrom tqdm import tqdm\n\nNUM_CLASSES = 30\n\nRANDOM_SEED = 2\nPATH_TO_DATA = '../input/vk-made-sports-image-classification'\nPATH_TO_TRAIN_CSV = PATH_TO_DATA + '/train.csv'\nPATH_TO_TEST_CSV = PATH_TO_DATA + '/test.csv'\nPATH_TO_TRAIN_IMAGES = PATH_TO_DATA + '/train'\nPATH_TO_TEST_IMAGES = PATH_TO_DATA + '/test'\n\nIMAGE_SIZE = 224\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-16T12:59:48.931886Z","iopub.execute_input":"2023-04-16T12:59:48.932866Z","iopub.status.idle":"2023-04-16T12:59:48.950454Z","shell.execute_reply.started":"2023-04-16T12:59:48.932812Z","shell.execute_reply":"2023-04-16T12:59:48.948623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(PATH_TO_TRAIN_CSV)\n\nlabel2id = {}\nid2label = {}\nfor i, label in enumerate(train_data.label.unique(), start=0):\n    label2id[label] = i\n    id2label[i] = label\nlabel2id","metadata":{"execution":{"iopub.status.busy":"2023-04-16T12:59:49.291027Z","iopub.execute_input":"2023-04-16T12:59:49.291790Z","iopub.status.idle":"2023-04-16T12:59:49.345332Z","shell.execute_reply.started":"2023-04-16T12:59:49.291752Z","shell.execute_reply":"2023-04-16T12:59:49.344252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"# для работы с датасетом\nclass ImageDataset(Dataset):\n    def __init__(self, img_dir, csv_file, transform=None, target_transform=None, test=False):\n        self.img_dir = img_dir\n        self.data = pd.read_csv(csv_file)\n        self.transform = transform\n        self.test = test\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.data.iloc[idx, 0])\n        image = Image.open(img_path).convert('RGB')\n        label = label2id[self.data.iloc[idx, 1]] if not self.test else self.data.iloc[idx, 0]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n    \n# Вывод изображений\ndef show_images(dataset, n_images, test=False):\n    np.random.seed(RANDOM_SEED)\n    fig, axes = plt.subplots(nrows=1, ncols=n_images, figsize=(15, 5))\n\n    for i in range(n_images):\n        idx = np.random.randint(0, len(dataset))\n        image, label_id = dataset[idx]\n        class_name = id2label[label_id] if not test else label_id\n        image_size = tuple(image.shape[1:])\n        axes[i].imshow(image.permute(1, 2, 0), vmin=0, vmax=1)\n        axes[i].set_title(f\"Class: {class_name}\\nSize: {image_size}\")\n        axes[i].axis('off')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T12:59:50.408973Z","iopub.execute_input":"2023-04-16T12:59:50.409906Z","iopub.status.idle":"2023-04-16T12:59:50.421831Z","shell.execute_reply.started":"2023-04-16T12:59:50.409854Z","shell.execute_reply":"2023-04-16T12:59:50.420699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Посмотрим на изображения и аугментации","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN,std=STD)\n])\n\ntrain_dataset = ImageDataset(PATH_TO_TRAIN_IMAGES, PATH_TO_TRAIN_CSV, transform=transform)\nshow_images(train_dataset, 5)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T12:59:51.617794Z","iopub.execute_input":"2023-04-16T12:59:51.618500Z","iopub.status.idle":"2023-04-16T12:59:52.155526Z","shell.execute_reply.started":"2023-04-16T12:59:51.618462Z","shell.execute_reply":"2023-04-16T12:59:52.154405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(35),\n    transforms.ColorJitter(brightness=0.6, contrast=0.4, saturation=0.6, hue=0.2),\n    transforms.Resize(IMAGE_SIZE + 30), # Рессайзим до большего размера чем нужно, чтобы поубирать черные места после поворота\n    transforms.CenterCrop(IMAGE_SIZE), # Берем квадрат из центра\n    transforms.RandomApply([\n        transforms.GaussianBlur(kernel_size=3)\n    ], p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN,std=STD)\n])\n\nall_train_dataset = ImageDataset(PATH_TO_TRAIN_IMAGES, PATH_TO_TRAIN_CSV, transform=train_transforms)\nshow_images(all_train_dataset, 5)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T13:00:57.433625Z","iopub.execute_input":"2023-04-16T13:00:57.434384Z","iopub.status.idle":"2023-04-16T13:00:57.906427Z","shell.execute_reply.started":"2023-04-16T13:00:57.434347Z","shell.execute_reply":"2023-04-16T13:00:57.905487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE + 30),\n    transforms.CenterCrop(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN,std=STD)\n])\n\ntest_dataset = ImageDataset(PATH_TO_TEST_IMAGES, PATH_TO_TEST_CSV, transform=test_transforms, test=True)\nshow_images(test_dataset, 5, test=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T13:01:03.171179Z","iopub.execute_input":"2023-04-16T13:01:03.171884Z","iopub.status.idle":"2023-04-16T13:01:03.555453Z","shell.execute_reply.started":"2023-04-16T13:01:03.171845Z","shell.execute_reply":"2023-04-16T13:01:03.554515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T13:01:04.250691Z","iopub.execute_input":"2023-04-16T13:01:04.251685Z","iopub.status.idle":"2023-04-16T13:01:04.259700Z","shell.execute_reply.started":"2023-04-16T13:01:04.251639Z","shell.execute_reply":"2023-04-16T13:01:04.258415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T13:01:05.020060Z","iopub.execute_input":"2023-04-16T13:01:05.021076Z","iopub.status.idle":"2023-04-16T13:01:05.028236Z","shell.execute_reply.started":"2023-04-16T13:01:05.021027Z","shell.execute_reply":"2023-04-16T13:01:05.027039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 512\n\n# Разделение датасета на train и val\ntrain_size = int(0.8 * len(all_train_dataset))\nval_size = len(all_train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(all_train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(RANDOM_SEED))\n\n# Создание train и val DataLoader\nall_train_loader = DataLoader(all_train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T13:01:05.918761Z","iopub.execute_input":"2023-04-16T13:01:05.919516Z","iopub.status.idle":"2023-04-16T13:01:05.933331Z","shell.execute_reply.started":"2023-04-16T13:01:05.919477Z","shell.execute_reply":"2023-04-16T13:01:05.932076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_loader), len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T13:01:14.901913Z","iopub.execute_input":"2023-04-16T13:01:14.902301Z","iopub.status.idle":"2023-04-16T13:01:14.909540Z","shell.execute_reply.started":"2023-04-16T13:01:14.902267Z","shell.execute_reply":"2023-04-16T13:01:14.908386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine tuning","metadata":{"execution":{"iopub.status.busy":"2023-04-02T17:57:08.778251Z","iopub.execute_input":"2023-04-02T17:57:08.779598Z","iopub.status.idle":"2023-04-02T17:57:08.785248Z","shell.execute_reply.started":"2023-04-02T17:57:08.779543Z","shell.execute_reply":"2023-04-02T17:57:08.784044Z"}}},{"cell_type":"code","source":"# функция для обучения модели\ndef train(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = []\n    running_micro_f1 = []\n    tqdm_loader = tqdm(enumerate(loader, 0), total=len(loader))\n    for i, data in tqdm_loader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        batch_loss = loss.item()\n        running_loss.append(batch_loss)\n        preds = torch.argmax(outputs, axis=1)\n        batch_f1 = f1_score(labels.cpu(), preds.cpu(), average='micro')\n        running_micro_f1.append(batch_f1)\n        tqdm_loader.set_description(f\"loss: {batch_loss:.3f}, F1: {batch_f1:.3f}\")\n\n    return running_loss, running_micro_f1\n\n# функция для валидации модели\ndef validate(model, loader, criterion, device):\n    model.eval()\n    running_loss = []\n    running_micro_f1 = []\n    tqdm_loader = tqdm(enumerate(loader, 0), total=len(loader))\n\n    with torch.no_grad():\n        for i, data in tqdm_loader:\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            batch_loss = loss.item()\n            running_loss.append(batch_loss)\n            preds = torch.argmax(outputs, axis=1)\n            batch_f1 = f1_score(labels.cpu(), preds.cpu(), average='micro')\n            running_micro_f1.append(batch_f1)\n            tqdm_loader.set_description(f\"loss: {batch_loss:.3f}, F1: {batch_f1:.3f}\")\n\n    return running_loss, running_micro_f1\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-16T13:01:17.714233Z","iopub.execute_input":"2023-04-16T13:01:17.715424Z","iopub.status.idle":"2023-04-16T13:01:17.728087Z","shell.execute_reply.started":"2023-04-16T13:01:17.715374Z","shell.execute_reply":"2023-04-16T13:01:17.726929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCHS = 45\n\nmodel = models.efficientnet_v2_m(pretrained=True)\n# model = models.vit_b_32()\n\nin_features = model.classifier[-1].in_features\nmodel.classifier = nn.Sequential(\n    nn.Dropout(0.2),\n    nn.Linear(in_features, NUM_CLASSES),\n)\n# Замораживаем все слои кроме последнего полносвязного\nfor name, param in model.named_parameters():\n    if name.startswith('features.8') or name.startswith('features.7') or name.startswith('classifier.') :\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\n#     print(name, param.requires_grad)\n    \nnum_params = sum(p.numel() for p in model.parameters())\nprint(f\"Number of parameters: {num_params}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-16T13:03:54.717509Z","iopub.execute_input":"2023-04-16T13:03:54.718449Z","iopub.status.idle":"2023-04-16T13:03:55.763255Z","shell.execute_reply.started":"2023-04-16T13:03:54.718397Z","shell.execute_reply":"2023-04-16T13:03:55.762038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/working/model_ef2_v2_m_3l_29.pth'))\n\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5, verbose=True)\n\ntrain_loss_history = []\ntrain_micro_f1_history = []\nval_loss_history = []\nval_micro_f1_history = []","metadata":{"execution":{"iopub.status.busy":"2023-04-16T16:09:53.015848Z","iopub.execute_input":"2023-04-16T16:09:53.016229Z","iopub.status.idle":"2023-04-16T16:09:53.875426Z","shell.execute_reply.started":"2023-04-16T16:09:53.016197Z","shell.execute_reply":"2023-04-16T16:09:53.874280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(30, NUM_EPOCHS):\n    print(f\"Epoch {epoch}\")\n\n    train_losses, train_micro_f1 = train(model, train_loader, criterion, optimizer, device)\n    train_loss = np.mean(train_losses)\n    train_f1 = np.mean(train_micro_f1)\n\n    val_losses, val_micro_f1 = validate(model, val_loader, criterion, device)\n    val_loss = np.mean(val_losses)\n    val_f1 = np.mean(val_micro_f1)\n\n    clear_output(True)\n\n    print(f\"Avg Train loss: {train_loss:.3f}, Avg Train micro F1: {train_f1:.3f}\")\n    \n    print(f\"Avg Val loss: {val_loss:.3f}, Avg Val micro F1: {val_f1:.3f}\")\n    scheduler.step()\n    \n    train_loss_history += train_losses\n    train_micro_f1_history += train_micro_f1\n    val_loss_history += val_losses\n    val_micro_f1_history += val_micro_f1\n\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(12, 4))\n\n    ax1.plot(train_loss_history, label='loss')\n    ax2.plot(val_loss_history, label='val loss')\n\n    ax3.plot(train_micro_f1_history, label='f1')\n    ax4.plot(val_micro_f1_history, label='val f1')\n\n    plt.show()\n    torch.save(model.state_dict(), f'model_ef2_v2_m_3l_{epoch}.pth')\n\n    submission = pd.read_csv(PATH_TO_TEST_CSV)\n    \n    submission = submission.set_index('image_id')\n    submission['label'] = -1\n\n    model.eval()\n\n    for image, pathes in tqdm(test_loader):\n        image = image.to(device)\n        # Вычисление предсказания\n        with torch.no_grad():\n            prediction = model(image)\n        labels = torch.argmax(prediction, dim=1)\n        for i, path in enumerate(pathes):\n            submission.loc[path, 'label'] = id2label[labels[i].item()]\n    submission = submission.reset_index()\n    assert (submission.label == -1).sum() == 0\n\n    submission.to_csv(f'submission_ef2_v2_m_3l_{epoch}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T16:09:59.776672Z","iopub.execute_input":"2023-04-16T16:09:59.777826Z","iopub.status.idle":"2023-04-16T16:44:41.230881Z","shell.execute_reply.started":"2023-04-16T16:09:59.777779Z","shell.execute_reply":"2023-04-16T16:44:41.228987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(PATH_TO_TEST_CSV)\nsubmission = submission.set_index('image_id')\nsubmission['label'] = -1\n\nmodel.eval()\n\nfor image, pathes in tqdm(test_loader):\n    image = image.to(device)\n    # Вычисление предсказания\n    with torch.no_grad():\n        prediction = model(image)\n    labels = torch.argmax(prediction, dim=1)\n    for i, path in enumerate(pathes):\n        submission.loc[path, 'label'] = id2label[labels[i].item()]\nsubmission = submission.reset_index()\nassert (submission.label == -1).sum() == 0\n\nsubmission.to_csv(f'submission_ef2_v2_m_3l_{epoch}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T13:24:51.701467Z","iopub.execute_input":"2023-04-16T13:24:51.701858Z","iopub.status.idle":"2023-04-16T13:28:30.759358Z","shell.execute_reply.started":"2023-04-16T13:24:51.701824Z","shell.execute_reply":"2023-04-16T13:28:30.758180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}